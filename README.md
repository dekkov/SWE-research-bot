
# OpenAI Software Engineer Job Requirement Analyzer

> **Status:** ‚úÖ Fully functional and tested with 80 jobs | 100% extraction success rate | $0.036 total LLM cost

## 1. Overview

An automated tool that collects Software Engineer job postings from OpenAI's careers website, extracts job requirements using LLM-based parsing, categorizes roles, and produces summarized requirement profiles.

**Key Features:**
- üîç Automated job discovery via browser automation
- ü§ñ LLM-powered requirement extraction and categorization
- üéØ Tech stack search and filtering
- üìä Web dashboard for browsing and analysis
- üì• Selective job export functionality
- üíæ SQLite-based persistent storage

Because OpenAI's careers site does not provide a public API, the system uses Playwright for browser automation to navigate dynamic pages, handle infinite scrolling, and extract structured data.

### System Status: ‚úÖ Fully Functional

**Tested and verified** with 80 OpenAI job postings:
- ‚úÖ 100% description extraction success rate
- ‚úÖ All 80 jobs parsed with tech stacks (avg 5.5 techs/job)
- ‚úÖ Web dashboard operational with search/filter capabilities
- ‚úÖ Total LLM cost: ~$0.036 for full pipeline

**Critical Implementation Notes:**
- The system handles client-side rendered (SPA) career pages using robust Playwright timing strategies
- See `Behavioural.md` for detailed debugging case study documenting race condition fixes
- See `CLAUDE.md` for architecture and critical implementation details

---

## 2. Goals and Non-Goals

### 2.1 Goals

- Automatically discover all OpenAI Software Engineer job postings
- Navigate individual job pages and extract full job descriptions
- Parse and structure job requirements using LLM (skills, experience, responsibilities)
- Categorize jobs into engineering role types
- Generate concise summaries of requirements per job category
- **Search jobs by technology stack/skills**
- **Web dashboard for browsing, filtering, and exporting jobs**
- **Manual trigger for scraping (on-demand, not scheduled)**
- **Export selected jobs in multiple formats (JSON, CSV, Markdown)**
- Avoid revisiting already-processed jobs
- Support resumable and repeatable runs  

### 2.2 Non-Goals

- Submitting applications or interacting with forms  
- Real-time monitoring of job changes  
- Scraping personal or applicant data  
- High-frequency or large-scale web crawling  

---

## 3. System Architecture

### 3.1 High-Level Flow

Search Page Loader  
‚Üí Job Link Extractor  
‚Üí Visited Job Tracker  
‚Üí Job Detail Navigator  
‚Üí Description Extractor  
‚Üí Requirement Parser  
‚Üí Job Categorizer  
‚Üí Category Summarizer  
‚Üí Output Generator


---

## 4. Technology Stack

### 4.1 Core Technologies

- **Language:** Python 3.10+
- **Browser Automation:** Playwright (headless Chrome/Firefox)
- **LLM Integration:** OpenAI API (GPT-4o-mini for parsing, categorization, summarization)
- **Data Storage:** SQLite (persistent job storage and deduplication)
- **Web Framework:** FastAPI (REST API + web dashboard)
- **Configuration:** python-dotenv (.env file management)
- **CLI:** Click (command-line interface)

### 4.2 Key Dependencies

```
playwright==1.40.0          # Browser automation
openai==1.3.0              # LLM integration
fastapi==0.104.1           # Web framework
uvicorn==0.24.0            # ASGI server
pydantic==2.5.0            # Data validation
pydantic-settings==2.1.0   # Settings management
python-dotenv==1.0.0       # Environment variables
click==8.1.7               # CLI framework
aiosqlite==0.19.0          # Async SQLite
jinja2==3.1.2              # HTML templates
```

### 4.3 Why This Minimal Stack?

**LLM-First Approach:**
- Instead of complex rule-based NLP (spaCy, regex, skill dictionaries), we use GPT-4o-mini's built-in understanding
- Single LLM call extracts all requirements with better accuracy and less code
- Easier to maintain and extend
- Cost-effective: GPT-4o-mini is ~15x cheaper than GPT-4 Turbo while maintaining high accuracy for parsing tasks

**SQLite Over JSONL:**
- Efficient deduplication and resumable runs
- Easy querying for search/filter features
- Low overhead for local usage  

---

## 5. Functional Components

### 5.1 Job Discovery Module (`src/scraper/discovery.py`)

**Purpose**
Load the OpenAI careers search page and discover all Software Engineer job listings.

**Responsibilities**
- Navigate to: `https://openai.com/careers/search/?q=Software+Engineer`
- Implement infinite scroll detection (scroll until no new jobs appear)
- Extract all job posting links and titles
- Normalize URLs and generate unique hashes for deduplication

**Implementation**
```python
async def discover_jobs(page):
    # Navigate to careers page
    # Scroll loop: scroll down, wait, check for new jobs
    # Extract job links and metadata
    # Return list of {url, title, hash}
```

**Key Challenges**
- Detecting when scrolling is complete
- Dynamic content loading delays
- Rate limiting and politeness delays  

---

### 5.2 Visited Job Tracker (`src/storage/database.py`)

**Purpose**
Prevent duplicate processing and support resumable runs.

**Implementation**
- Generate SHA-256 hash of canonical job URL as unique identifier
- Check SQLite `jobs` table for existing `job_url_hash` before scraping
- Mark jobs as `processed=TRUE` after successful analysis

**Database Query**
```sql
SELECT COUNT(*) FROM jobs WHERE job_url_hash = ?
```

**Benefits**
- Fast lookups with indexed hash column
- Resumable scraping after interruptions
- Historical tracking of all discovered jobs  

---

### 5.3 Job Detail Extraction Module (`src/scraper/extractor.py`)

**Purpose**
Navigate to individual job pages and extract complete job descriptions.

**Responsibilities**
- Navigate to job detail page with retry logic
- **CRITICAL**: Wait for JavaScript to fully render content (see timing strategy below)
- Scroll to trigger any lazy-loaded content
- Extract metadata (title, team, location, employment type)
- Extract full job description HTML/text

**Implementation** (with robust timing for SPAs):
```python
async def extract_job_details(page, job_url):
    # Wait for network idle (JavaScript has executed)
    await page.goto(job_url, wait_until="networkidle", timeout=60000)
    await page.wait_for_timeout(3000)

    # Scroll to trigger lazy-loaded content
    await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
    await page.wait_for_timeout(2000)

    # Wait for description to be VISIBLE (not just in DOM)
    await page.wait_for_selector('.prose', state='visible', timeout=15000)

    return {
        'title': await page.text_content('h1'),
        'description': await page.text_content('.prose'),
        'metadata': {...}
    }
```

**Critical Lesson Learned:**
Using `wait_until="domcontentloaded"` caused 95%+ extraction failures because it fires before JavaScript renders the content. Modern SPAs require `wait_until="networkidle"` + `state='visible'` checks. See `Behavioural.md` for full debugging case study.

---

### 5.4 LLM-Based Requirement Parser (`src/analyzer/parser.py`)

**Purpose**
Convert unstructured job descriptions into structured requirement data using OpenAI GPT-4o-mini.

**Single LLM Call Approach**
Instead of complex rule-based NLP, we use a single GPT-4o-mini API call with a structured prompt:

```python
prompt = f"""
Extract and structure the following information from this job posting:

Job Description:
{job_description}

Return a JSON object with these fields:
{{
  "responsibilities": [...],      // Key responsibilities
  "required_skills": [...],        // Must-have technical skills
  "preferred_skills": [...],       // Nice-to-have skills
  "experience": [...],             // Years of experience, domain exp
  "education": [...]               // Degree requirements
}}
"""

response = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": prompt}],
    response_format={"type": "json_object"}
)
```

**Advantages**
- No regex maintenance
- Handles varied job posting formats automatically
- Better skill/technology extraction
- Understands context and implicit requirements
    

---

### 5.5 LLM-Based Job Categorization (`src/analyzer/categorizer.py`)

**Purpose**
Assign each job to one or more engineering categories using GPT-4o-mini classification.

**Predefined Categories**
- Backend Engineer
- Frontend Engineer
- Full Stack Engineer
- Infrastructure/Platform
- Machine Learning/AI
- Systems/Performance
- Security/Reliability
- Mobile Engineer

**LLM Classification Prompt**
```python
prompt = f"""
Based on this job title and requirements, categorize this role.

Job Title: {title}
Required Skills: {skills}
Responsibilities: {responsibilities}

Categories: {CATEGORIES}

Return JSON:
{{
  "primary_category": "...",
  "secondary_categories": [...],
  "confidence": 0.95
}}
"""
```

**Database Storage**
Each job can have multiple categories (many-to-many relationship via `job_categories` table).

---

### 5.6 Category Summarization Module (`src/analyzer/summarizer.py`)

**Purpose**
Produce aggregated summaries of requirements for each job category.

**Process**
1. **Aggregate by Category**: Group all jobs in same category
2. **Skill Frequency Analysis**: Count and rank all mentioned skills
3. **LLM Summary Generation**: Generate narrative summary

**LLM Summarization Prompt**
```python
prompt = f"""
Summarize the requirements for {category} roles at OpenAI.

Jobs analyzed: {job_count}
Most common skills: {top_skills}
All responsibilities: {all_responsibilities}

Generate a concise summary covering:
1. Core technologies and tools
2. Typical responsibilities
3. Experience levels required
4. Domain expertise needed
"""
```

**Storage**
Summaries stored in `category_summaries` table with `last_updated` timestamp.

---

### 5.7 Tech Stack Search Module (`src/web/search.py`)

**Purpose**
Enable searching jobs by specific technologies, languages, frameworks, or skills.

**Implementation**
```sql
SELECT DISTINCT j.*, r.required_skills, r.preferred_skills
FROM jobs j
JOIN requirements r ON j.id = r.job_id
WHERE r.required_skills LIKE '%Python%'
   OR r.preferred_skills LIKE '%Python%'
```

**Search Features**
- Full-text search across skills and technologies
- Filter by category + tech stack (e.g., "Backend + Rust")
- Boolean operators (AND, OR)
- Skill matching with synonyms (e.g., "React" ‚Üí "React.js", "ReactJS")

**Web UI**
- Search bar with autocomplete for common technologies
- Tech stack tags/chips for filtering
- Real-time result updates

---

### 5.8 Selective Export Module (`src/web/export.py`)

**Purpose**
Export selected jobs or filtered results in multiple formats.

**Supported Formats**
- **JSON**: Full structured data including requirements
- **CSV**: Tabular format for spreadsheet analysis
- **Markdown**: Human-readable format with formatting

**Export Options**
- Export all jobs
- Export filtered/searched results
- Export selected jobs (checkboxes in web UI)
- Export by category

**Implementation**
```python
def export_jobs(job_ids, format='json'):
    jobs = fetch_jobs_with_requirements(job_ids)

    if format == 'json':
        return json.dumps(jobs, indent=2)
    elif format == 'csv':
        return convert_to_csv(jobs)
    elif format == 'markdown':
        return generate_markdown_report(jobs)
```

---

## 6. Project Structure

```
SWE-research-bot/
‚îú‚îÄ‚îÄ .env.example              # Environment variables template
‚îú‚îÄ‚îÄ .env                      # Your local config (gitignored)
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ main.py                   # CLI entry point
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py           # Configuration loader
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ browser.py        # Playwright browser manager
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ discovery.py      # Job listing discovery
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ extractor.py      # Job detail extraction
‚îÇ   ‚îú‚îÄ‚îÄ analyzer/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py     # OpenAI API wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py         # Requirement parsing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ categorizer.py    # Job categorization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summarizer.py     # Category summaries
‚îÇ   ‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py       # SQLite operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py         # Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commands.py       # Click CLI commands
‚îÇ   ‚îî‚îÄ‚îÄ web/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ app.py            # FastAPI application
‚îÇ       ‚îú‚îÄ‚îÄ api/              # REST API endpoints
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ jobs.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ search.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ export.py
‚îÇ       ‚îú‚îÄ‚îÄ static/           # CSS, JS files
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ style.css
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ app.js
‚îÇ       ‚îî‚îÄ‚îÄ templates/        # Jinja2 HTML templates
‚îÇ           ‚îú‚îÄ‚îÄ index.html
‚îÇ           ‚îú‚îÄ‚îÄ jobs.html
‚îÇ           ‚îú‚îÄ‚îÄ job_detail.html
‚îÇ           ‚îî‚îÄ‚îÄ categories.html
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ jobs.db              # SQLite database (auto-created)
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ scraper.log          # Application logs
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ test_scraper.py
    ‚îú‚îÄ‚îÄ test_analyzer.py
    ‚îî‚îÄ‚îÄ test_api.py
```

---

## 7. CLI Commands

```bash
# Install dependencies and setup
pip install -r requirements.txt
playwright install chromium
cp .env.example .env
# Edit .env with your OPENAI_API_KEY

# Initialize database
python main.py init

# Scrape jobs (manual trigger)
python main.py scrape

# Analyze scraped jobs (parse requirements, categorize)
python main.py analyze

# Generate category summaries
python main.py summarize

# Run full pipeline (scrape + analyze + summarize)
python main.py run-all

# Show statistics
python main.py status

# Export jobs
python main.py export --format json --output jobs.json
python main.py export --format csv --category "Backend Engineer"
python main.py export --format markdown --tech-stack "Python,Rust"

# Launch web dashboard
python main.py web
# Visit http://localhost:8000
```

---

## 8. Web Dashboard Features

### 8.1 Dashboard Home (`/`)
- **Statistics Overview**
  - Total jobs scraped
  - Jobs by category (pie chart)
  - Last scrape timestamp
  - Top 10 most common skills

- **Quick Actions**
  - üîÑ **Trigger Scrape** button (starts scrape job)
  - üìä View categories
  - üîç Search jobs
  - üì• Export all jobs

### 8.2 Jobs Browser (`/jobs`)
- **Features**
  - Paginated job list with cards
  - Checkbox selection for bulk export
  - Filter by category dropdown
  - **Tech stack search bar** with autocomplete
  - Sort by date, title, category

- **Job Card Display**
  - Job title and team
  - Location and employment type
  - Primary category badge
  - Top 5 required skills as tags
  - Link to full details

### 8.3 Job Detail View (`/jobs/{id}`)
- Full job description
- Structured requirements:
  - Responsibilities (bullet list)
  - Required skills (tags)
  - Preferred skills (tags)
  - Experience requirements
  - Education requirements
- Categories with confidence scores
- Link to original OpenAI posting
- Export this job button

### 8.4 Category View (`/categories`)
- List of all categories
- For each category:
  - Job count
  - Top 10 skills (frequency bar chart)
  - LLM-generated summary
  - "View jobs" link

### 8.5 Search & Filter (`/search`)
- **Tech Stack Search**
  - Multi-select technology tags
  - Boolean logic (AND/OR toggle)
  - Search across required + preferred skills

- **Combined Filters**
  - Category + tech stack
  - Location + tech stack
  - Experience level + skills

- **Export Results**
  - Export filtered jobs only
  - Select specific jobs from results

### 8.6 API Endpoints

```
GET  /api/jobs                    # List jobs (with filters)
GET  /api/jobs/{id}               # Get job details
POST /api/scrape/trigger          # Trigger scrape job
GET  /api/scrape/status           # Check scrape progress
GET  /api/categories              # List categories with summaries
GET  /api/search?tech=Python,Rust # Search by tech stack
POST /api/export                  # Export selected jobs
  Body: {
    "job_ids": [1, 2, 3],
    "format": "json|csv|markdown"
  }
```

---

## 9. Database Schema

```sql
-- Jobs table
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_url TEXT UNIQUE NOT NULL,
    job_url_hash TEXT UNIQUE NOT NULL,
    title TEXT NOT NULL,
    team TEXT,
    location TEXT,
    employment_type TEXT,
    raw_description TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed BOOLEAN DEFAULT FALSE
);

CREATE INDEX idx_job_url_hash ON jobs(job_url_hash);
CREATE INDEX idx_processed ON jobs(processed);

-- Requirements table
CREATE TABLE requirements (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id INTEGER NOT NULL,
    responsibilities TEXT,      -- JSON array
    required_skills TEXT,        -- JSON array
    preferred_skills TEXT,       -- JSON array
    experience TEXT,             -- JSON array
    education TEXT,              -- JSON array
    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
);

CREATE INDEX idx_requirements_job_id ON requirements(job_id);

-- Job categories (many-to-many)
CREATE TABLE job_categories (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id INTEGER NOT NULL,
    category TEXT NOT NULL,
    is_primary BOOLEAN DEFAULT FALSE,
    confidence REAL,
    FOREIGN KEY (job_id) REFERENCES jobs(id) ON DELETE CASCADE
);

CREATE INDEX idx_job_categories_job_id ON job_categories(job_id);
CREATE INDEX idx_job_categories_category ON job_categories(category);

-- Category summaries
CREATE TABLE category_summaries (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    category TEXT UNIQUE NOT NULL,
    job_count INTEGER,
    core_technologies TEXT,        -- JSON array
    common_responsibilities TEXT,  -- Text summary
    experience_levels TEXT,        -- Text summary
    summary TEXT,                  -- Full LLM-generated summary
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Scrape state tracking
CREATE TABLE scrape_runs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    jobs_discovered INTEGER DEFAULT 0,
    jobs_scraped INTEGER DEFAULT 0,
    jobs_analyzed INTEGER DEFAULT 0,
    status TEXT,  -- running, completed, failed
    error_message TEXT
);
```

---

## 10. Setup and Installation

### Prerequisites
- Python 3.10+
- OpenAI API key

### Installation Steps

1. **Clone the repository**
```bash
git clone <repo-url>
cd SWE-research-bot
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
playwright install chromium
```

4. **Configure environment**
```bash
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

5. **Initialize database**
```bash
python main.py init
```

6. **Run your first scrape**
```bash
python main.py run-all
```

7. **Launch web dashboard**
```bash
python main.py web
# Open http://localhost:8000 in your browser
```

---

## 11. Usage Examples

### Example 1: Find all Backend jobs requiring Rust
```bash
# CLI
python main.py export --category "Backend Engineer" --tech-stack "Rust" --format markdown

# Web Dashboard
Navigate to /search ‚Üí Select "Backend Engineer" ‚Üí Type "Rust" ‚Üí Export Results
```

### Example 2: Compare ML Engineer requirements across jobs
```bash
# CLI
python main.py summarize --category "Machine Learning/AI"

# Web Dashboard
Navigate to /categories ‚Üí Click "Machine Learning/AI" ‚Üí View summary
```

### Example 3: Export specific jobs for application tracking
```bash
# Web Dashboard
Navigate to /jobs ‚Üí Select interesting jobs (checkboxes) ‚Üí Click "Export Selected" ‚Üí Choose JSON
# Import JSON into your application tracker
```

---

## 12. Cost Estimation

### OpenAI API Costs (GPT-4o-mini)
**Actual costs based on 80 OpenAI job postings processed:**

| Operation | Input Tokens | Output Tokens | Cost/Job | Total (80 jobs) |
|-----------|--------------|---------------|----------|-----------------|
| Requirement Parsing | ~1,400 | ~200 | $0.00033 | $0.026 |
| Categorization | ~400 | ~80 | $0.00011 | $0.009 |
| Category Summaries (3 categories) | ~400 | ~100 | - | $0.001 |
| **Total** | | | | **~$0.036** |

**GPT-4o-mini Pricing (December 2024):**
- Input: $0.15 per 1M tokens
- Output: $0.60 per 1M tokens

**Cost Comparison:**
- GPT-4o-mini: **$0.036 for 80 jobs** (~$0.00045 per job)
- GPT-4 Turbo: Would cost ~$20 for 80 jobs (~$0.25 per job)
- **Savings: ~560x cheaper** while maintaining high parsing accuracy

*Costs are one-time per scrape run. Resumable runs won't re-process existing jobs.*

---

## 13. Future Enhancements

- [ ] Email notifications when new jobs matching criteria are posted
- [ ] Chrome extension for one-click job saving
- [ ] Compare OpenAI jobs with other companies (Google, Anthropic, etc.)
- [ ] Salary range estimation based on requirements
- [ ] Skill gap analysis (compare your skills vs job requirements)
- [ ] Application tracking integration
- [ ] Resume tailoring suggestions per job

---

## 14. License

MIT License